{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7f3cdf1-4d49-418a-b91e-2ddcc4d5fdac",
   "metadata": {},
   "source": [
    "Esse notebook automatiza a conversão de um lote de arquivos PDF localizados em um diretório específico em imagens JPG.\n",
    "\n",
    "Ele utiliza threads para um processamento mais rápido e inclui um mecanismo para retomar a conversão em caso de interrupções.\n",
    "Para cada imagem processada, os metadados são lidos de um arquivo CSV (`selected_records_with_downloaded_pdfs.csv`), e um novo arquivo CSV (`records_with_images_metadata.csv`) é criado contendo campos de metadados específicos.\n",
    "    \n",
    "### 1. Importação de Bibliotecas Essenciais\n",
    "Esta seção importa todas as bibliotecas necessárias para as operações deste notebook.\n",
    "\n",
    "Elas são organizadas para facilitar a compreensão e manutenção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a777e9c2-d666-4adf-9874-1748a43ecef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import traceback\n",
    "import threading \n",
    "from queue import Queue \n",
    "import logging           \n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import concurrent.futures  \n",
    "import pandas as pd       \n",
    "from pdf2image import convert_from_path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787ea423-751d-4ef6-b21a-81f96c2021f0",
   "metadata": {},
   "source": [
    "### 2. Configuração de variáveis\n",
    "Esta seção define as principais variáveis que controlam o comportamento do script de conversão de PDF para imagem e extração de metadados. \n",
    "\n",
    "**Você pode ajustar esses valores para personalizar a operação de acordo com suas necessidades.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc45b783-3d8e-4925-a556-3c74247ca13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório onde os arquivos PDF a serem convertidos estão localizados.\n",
    "PDF_DIR: str = \"PDFs\"\n",
    "\n",
    "# Diretório onde as imagens JPG resultantes da conversão serão salvas.\n",
    "JPG_OUTPUT_DIR: str = \"JPG_Images\"\n",
    "\n",
    "# Nome do arquivo CSV de entrada que contém os metadados originais e os caminhos dos PDFs baixados.\n",
    "# Este é o CSV atualizado do notebook anterior.\n",
    "RECORDS_CSV: str = \"selected_records_with_downloaded_pdfs.csv\"\n",
    "\n",
    "# Nome do arquivo CSV de saída onde os metadados das imagens processadas serão registrados.\n",
    "# Este arquivo será criado ou atualizado durante o processamento.\n",
    "METADATA_CSV: str = \"records_with_images_metadata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d993be-4704-481d-83bf-b5d67d2f8cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de threads (processos concorrentes) a serem usados para converter PDFs em imagens.\n",
    "# Um número maior pode acelerar a conversão, mas consome mais recursos do sistema (CPU/RAM).\n",
    "# Ajuste conforme a capacidade do seu hardware e a natureza dos PDFs.\n",
    "NUM_WORKERS: int = 10\n",
    "\n",
    "# Resolução (DPI - Dots Per Inch) para a conversão de PDF para imagem.\n",
    "# Valores mais altos resultam em imagens de maior qualidade e clareza, mas também aumentam\n",
    "# o tempo de processamento e o tamanho final dos arquivos JPG.\n",
    "# Sugestões: 72 (tela), 150 (boa qualidade para visualização), 300 (qualidade de impressão).\n",
    "DPI: int = 200\n",
    "\n",
    "# Tamanho do \"pedaço\" (chunk) de PDFs a serem processados por cada thread.\n",
    "# Isso afeta como as tarefas são distribuídas entre as threads.\n",
    "# Um chunk_size muito pequeno pode aumentar o overhead; um muito grande pode desequilibrar a carga.\n",
    "# Geralmente, um valor entre 10 e 50 é um bom ponto de partida.\n",
    "CHUNK_SIZE: int = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b291df-d7d2-47a2-a7cd-bf2b169f8ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(JPG_OUTPUT_DIR):\n",
    "    os.makedirs(JPG_OUTPUT_DIR)\n",
    "    print(f\"Diretório de saída para JPGs '{JPG_OUTPUT_DIR}' criado com sucesso.\")\n",
    "else:\n",
    "    print(f\"Diretório de saída para JPGs '{JPG_OUTPUT_DIR}' já existe.\")\n",
    "\n",
    "if not os.path.exists(PDF_DIR):\n",
    "    print(f\"AVISO: O diretório de PDFs de entrada '{PDF_DIR}' não foi encontrado.\")\n",
    "    print(\"Por favor, verifique se o caminho está correto e se os PDFs foram baixados para lá.\")\n",
    "else:\n",
    "    print(f\"Diretório de PDFs de entrada '{PDF_DIR}' verificado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fda842e-cd53-43f9-94df-0aa727ef0dc8",
   "metadata": {},
   "source": [
    "### 3. Configuração do Sistema de Logging\n",
    "\n",
    "Esta seção configura o sistema de log do Python para registrar o progresso das conversões, avisos e quaisquer erros em um arquivo dedicado (`pdf_conversion.log`). \n",
    "\n",
    "Isso é extremamente útil para monitorar a execução, depurar problemas e ter um histórico completo do processo, mesmo que o notebook seja fechado ou interrompido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4d5d5-b1bf-40e9-b2fa-44875032ee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FILE: str = \"pdf_conversion.log\"\n",
    "\n",
    "logging.basicConfig(filename=LOG_FILE, level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(threadName)s - %(message)s')\n",
    "\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(threadName)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "if not any(isinstance(handler, logging.StreamHandler) for handler in logging.getLogger().handlers):\n",
    "    logging.getLogger().addHandler(console_handler)\n",
    "\n",
    "logging.info(f\"Sistema de logging configurado. Logs serão gravados em '{LOG_FILE}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e95bd-e2eb-47aa-99bf-051fe2841d0a",
   "metadata": {},
   "source": [
    "### 4. Definindo Funções Auxiliares Essenciais\n",
    "Esta seção define as funções de apoio que realizam tarefas específicas e repetitivas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302d137b-67a1-4f06-8723-8470a05eb07d",
   "metadata": {},
   "source": [
    "#### 4.1. Verificação e Criação de Diretórios (ensure_directories_exist)\n",
    "\n",
    "Garante que todos os diretórios necessários para salvar os arquivos de saída existam antes que o processo de conversão comece, prevenindo erros de \"diretório não encontrado\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26c86c6-523d-4adc-9599-c0d4ccae37e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_directories_exist(directories: List[str]) -> None:\n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        logging.info(f\"Diretório '{directory}' garantido (criado ou já existente).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e18a93-8347-4a8c-a028-ae36c22fea44",
   "metadata": {},
   "source": [
    "#### 4.2. Carregamento dos Dados do CSV (load_records_csv)\n",
    "\n",
    "Esta função é responsável por carregar o arquivo CSV de entrada que contém os metadados.\n",
    "\n",
    "Ela inclui tratamento de erros para lidar com arquivos ausentes, vazios ou colunas essenciais faltando, garantindo que o processamento só continue se os dados estiverem válidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc8e07-b086-4b90-b4d5-dd7856468053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_records_csv(csv_path: str) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        df: pd.DataFrame = pd.read_csv(csv_path)\n",
    "        logging.info(f\"Arquivo CSV de registros '{csv_path}' carregado com sucesso. {len(df)} entradas encontradas.\")\n",
    "\n",
    "        required_columns = ['id', 'downloaded_pdf_filenames']\n",
    "        for col in required_columns:\n",
    "            if col not in df.columns:\n",
    "                logging.error(f\"Erro: Coluna essencial '{col}' não encontrada no arquivo CSV '{csv_path}'.\")\n",
    "                logging.error(\"Certifique-se de que o CSV é o resultado do primeiro notebook ou contém os caminhos dos PDFs.\")\n",
    "                return None\n",
    "\n",
    "        df['id'] = df['id'].astype(str)\n",
    "        \n",
    "        df['downloaded_pdf_filenames'] = df['downloaded_pdf_filenames'].apply(\n",
    "            lambda x: eval(x) if pd.notna(x) and isinstance(x, str) and x.strip().startswith('[') else []\n",
    "        )\n",
    "        logging.info(\"Coluna 'downloaded_pdf_filenames' convertida de string para lista de Python.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Erro: Arquivo CSV de registros não encontrado em '{csv_path}'.\")\n",
    "        return None\n",
    "    except pd.errors.EmptyDataError:\n",
    "        logging.error(f\"Erro: Arquivo CSV '{csv_path}' está vazio. Não há dados para processar.\")\n",
    "        return None\n",
    "    except pd.errors.ParserError as e:\n",
    "        logging.error(f\"Erro de parsing no arquivo CSV '{csv_path}': {e}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro inesperado ao carregar o CSV '{csv_path}': {e}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0f0067-c4bc-42ff-9a7f-70ab381af387",
   "metadata": {},
   "source": [
    "#### 4.3. Conversão de PDF para JPGs (pdf_to_jpgs)\n",
    "\n",
    "Esta função utiliza a biblioteca `pdf2image` para converter um único arquivo PDF em uma ou mais imagens JPG (uma para cada página).\n",
    "\n",
    "Ela retorna uma lista contendo o nome do arquivo JPG gerado e o número da página correspondente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e9f79e-bd6b-4072-a9a5-07b79c0bd12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_jpgs(pdf_path: str, output_dir: str, original_item_id: str, dpi: int) -> List[Tuple[str, int]]:\n",
    "    jpg_data: List[Tuple[str, int]] = []\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=dpi)\n",
    "\n",
    "        for i, image in enumerate(images):\n",
    "            if len(images) == 1:\n",
    "                jpg_filename = f\"{original_item_id}.jpg\"\n",
    "            else:\n",
    "                jpg_filename = f\"{original_item_id}_page_{i + 1}.jpg\"\n",
    "\n",
    "            jpg_path = os.path.join(output_dir, jpg_filename)\n",
    "\n",
    "            if not os.path.exists(jpg_path):\n",
    "                image.save(jpg_path, 'JPEG')\n",
    "                logging.info(f\"[{threading.current_thread().name}] Gerado JPG: '{jpg_path}'.\")\n",
    "            else:\n",
    "                logging.debug(f\"[{threading.current_thread().name}] JPG '{jpg_path}' já existe. Pulando salvamento.\")\n",
    "            \n",
    "            jpg_data.append((jpg_filename, i + 1))\n",
    "        \n",
    "        logging.info(f\"[{threading.current_thread().name}] PDF '{os.path.basename(pdf_path)}' convertido para {len(images)} JPG(s).\")\n",
    "        return jpg_data\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[{threading.current_thread().name}] Erro ao converter PDF '{pdf_path}' para JPGs: {e}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7463070-aa63-4d9e-88d5-857d792031fe",
   "metadata": {},
   "source": [
    "#### 4.4. Processamento de PDF Individual (process_pdf)\n",
    "\n",
    "Esta é a função central que orquestra o processamento de um único arquivo PDF. Ela:\n",
    "\n",
    "- Extrai o ID, verifica se o PDF já foi processado (para resumibilidade).\n",
    "- Chama a função de conversão.\n",
    "- Coleta os metadados correspondentes do DataFrame original.\n",
    "- Enfileira os metadados para serem gravados no CSV de saída.\n",
    "\n",
    "O uso de um Lock garante a segurança de threads ao atualizar o conjunto de IDs processados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c98f9-4056-4361-ba44-1f1fb2572237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_pdf_task(\n",
    "    pdf_path: str,\n",
    "    output_dir: str,\n",
    "    records_df: pd.DataFrame,\n",
    "    processed_ids_lock: threading.Lock,\n",
    "    processed_ids_set: set,\n",
    "    metadata_list_lock: threading.Lock,\n",
    "    global_metadata_list: List[Dict],\n",
    "    dpi: int\n",
    ") -> None:\n",
    "    \n",
    "    thread_name = threading.current_thread().name\n",
    "    logging.info(f\"[{thread_name}] Iniciando processamento para PDF: '{os.path.basename(pdf_path)}'.\")\n",
    "\n",
    "    try:\n",
    "        page_id_from_pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        \n",
    "        unique_download_id = page_id_from_pdf_name \n",
    "\n",
    "        logging.debug(f\"[{thread_name}] PDF: '{os.path.basename(pdf_path)}' -> Page ID from filename (Unique Download ID): '{unique_download_id}'\")\n",
    "\n",
    "        with processed_ids_lock:\n",
    "            if unique_download_id in processed_ids_set:\n",
    "                logging.info(f\"[{thread_name}] Pulando PDF já processado (pelo ID da página): '{os.path.basename(pdf_path)}'.\")\n",
    "                return\n",
    "        \n",
    "        original_item_id = None\n",
    "        row_metadata = None\n",
    "\n",
    "        pdf_filename_for_csv_check = os.path.join(\"PDFs\", os.path.basename(pdf_path))\n",
    "        \n",
    "        matching_rows = records_df[\n",
    "            records_df['downloaded_pdf_filenames'].apply(lambda x: pdf_filename_for_csv_check in x if isinstance(x, list) else False)\n",
    "        ]\n",
    "\n",
    "        if not matching_rows.empty:\n",
    "            row_metadata = matching_rows.iloc[0]\n",
    "            original_item_id = str(row_metadata['id']) # O ID do artigo\n",
    "            logging.debug(f\"[{thread_name}] Encontrado artigo pai '{original_item_id}' para PDF '{os.path.basename(pdf_path)}'.\")\n",
    "        else:\n",
    "            logging.warning(f\"[{thread_name}] Não foi possível encontrar o registro do artigo pai para o PDF '{os.path.basename(pdf_path)}' no 'records_df' usando a coluna 'downloaded_pdf_filenames'. Os metadados para este PDF não serão completos.\")\n",
    "            return \n",
    "\n",
    "        jpg_data: List[Tuple[str, int]] = pdf_to_jpgs(pdf_path, output_dir, page_id_from_pdf_name, dpi)\n",
    "        \n",
    "        if not jpg_data:\n",
    "            logging.warning(f\"[{thread_name}] Nenhuma imagem JPG gerada para o PDF: '{os.path.basename(pdf_path)}'. Pulando coleta de metadados.\")\n",
    "            return\n",
    "\n",
    "        new_metadata_entries = []\n",
    "        for jpg_filename, page_num in jpg_data:\n",
    "            metadata_entry = {\n",
    "                'original_article_id': original_item_id, \n",
    "                'page_id': page_id_from_pdf_name,        \n",
    "                'download_id': unique_download_id,       \n",
    "                'jpg_filename': jpg_filename,\n",
    "                'jpg_page_number': page_num,         \n",
    "                'trove_page_url': row_metadata.get('trove_page_url', 'N/A'), \n",
    "                'date': row_metadata.get('date', 'N/A'),\n",
    "                'page': row_metadata.get('page', 'N/A'),\n",
    "                'illustrated': row_metadata.get('illustrated', 'N/A'),\n",
    "                'type': row_metadata.get('type', 'N/A'),\n",
    "            }\n",
    "            new_metadata_entries.append(metadata_entry)\n",
    "        \n",
    "        with metadata_list_lock:\n",
    "            global_metadata_list.extend(new_metadata_entries)\n",
    "        \n",
    "        logging.info(f\"[{thread_name}] {len(new_metadata_entries)} metadados para PDF '{os.path.basename(pdf_path)}' (Artigo: '{original_item_id}') coletados e enfileirados.\")\n",
    "\n",
    "        with processed_ids_lock:\n",
    "            processed_ids_set.add(unique_download_id)\n",
    "            logging.debug(f\"[{thread_name}] ID único '{unique_download_id}' adicionado aos IDs processados.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[{thread_name}] Erro crítico ao processar PDF '{pdf_path}': {e}\")\n",
    "        logging.error(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9177a5f1-a58b-494f-b289-8cc7f0dda96a",
   "metadata": {},
   "source": [
    "#### 4.7. Obtenção de IDs Já Processados (get_already_processed_ids)\n",
    "\n",
    "Esta função é a base da capacidade de retomada do seu script. Ela:\n",
    "\n",
    "1. Verifica se o arquivo CSV de metadados de saída já existe.\n",
    "2. Se existir, ela lê a coluna 'id' desse arquivo e retorna um conjunto desses IDs.\n",
    "3. Este conjunto é então usado para pular PDFs que já foram convertidos em execuções anteriores, economizando tempo e recursos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8d8e52-0e8a-40db-a96e-6727025b4923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_already_processed_ids(metadata_file: str) -> set:\n",
    "    processed_ids_set: set = set()\n",
    "    if os.path.exists(metadata_file):\n",
    "        try:\n",
    "            existing_df: pd.DataFrame = pd.read_csv(metadata_file, dtype={'download_id': str})\n",
    "            \n",
    "            if 'download_id' in existing_df.columns:\n",
    "                processed_ids_set = set(existing_df['download_id'].unique())\n",
    "                logging.info(f\"Encontrados {len(processed_ids_set)} IDs de downloads já processados em '{metadata_file}'.\")\n",
    "            else:\n",
    "                logging.warning(f\"Arquivo de metadados '{metadata_file}' não contém a coluna 'download_id'.\")\n",
    "                logging.warning(\"Todos os PDFs serão processados novamente para garantir a integridade.\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            logging.info(f\"Arquivo de metadados '{metadata_file}' está vazio. Nenhum ID processado encontrado.\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Erro ao ler IDs já processados do arquivo '{metadata_file}': {e}.\")\n",
    "            logging.warning(\"Todos os PDFs serão processados novamente para garantir a integridade.\")\n",
    "            logging.debug(traceback.format_exc())\n",
    "    else:\n",
    "        logging.info(f\"Arquivo de metadados '{metadata_file}' não encontrado. Iniciando processamento do zero.\")\n",
    "\n",
    "    return processed_ids_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd83a148-215f-41ba-8ede-e1289e9de32d",
   "metadata": {},
   "source": [
    "### 5. Execução principal\n",
    "Esta seção define a função principal, que orquestra todo o processo de conversão de PDF e extração de metadados.\n",
    "\n",
    "Essa função principal configura as filas, os bloqueios e os threads necessários.\n",
    "- Ela preenche a fila `pdf_queue` com pedaços de caminhos de arquivos PDF e inicia os threads de trabalho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b240088-0f1b-45b8-8c73-b86827eeac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    pdf_dir: str,\n",
    "    jpg_output_dir: str,\n",
    "    records_csv_path: str,\n",
    "    metadata_csv_path: str,\n",
    "    num_workers: int,\n",
    "    dpi: int\n",
    ") -> None:\n",
    "    \n",
    "    logging.info(\"--- INICIANDO PROCESSO PRINCIPAL DE CONVERSÃO DE PDF PARA JPG (concurrent.futures) ---\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # 1. Garantir que os diretórios de saída existam\n",
    "    ensure_directories_exist([jpg_output_dir])\n",
    "\n",
    "    # 2. Carregar os metadados dos registros do CSV\n",
    "    records_df = load_records_csv(records_csv_path)\n",
    "    if records_df is None:\n",
    "        logging.critical(f\"Não foi possível carregar o arquivo CSV de registros '{records_csv_path}'. Encerrando.\")\n",
    "        return\n",
    "\n",
    "    # 3. Obter IDs já processados (para funcionalidade de retomada)\n",
    "    # processed_ids_set será um recurso compartilhado, acessado com lock.\n",
    "    processed_ids_set = get_already_processed_ids(metadata_csv_path)\n",
    "    logging.info(f\"Retomada ativada: {len(processed_ids_set)} PDFs já processados serão pulados.\")\n",
    "\n",
    "     # 4. Preparar a lista de PDFs a serem processados\n",
    "    pdf_paths_to_process: List[str] = []\n",
    "    skipped_due_to_non_existence = 0\n",
    "    skipped_due_to_already_processed = 0\n",
    "\n",
    "    print(\"\\nPreparando lista de PDFs elegíveis para conversão. Isso pode levar alguns segundos...\")\n",
    "    for _, row in tqdm(records_df.iterrows(), total=len(records_df), desc=\"Verificando PDFs do CSV\"):\n",
    "        if isinstance(row['downloaded_pdf_filenames'], list) and row['downloaded_pdf_filenames']:\n",
    "            for pdf_filename_in_csv in row['downloaded_pdf_filenames']:\n",
    "                full_pdf_path = os.path.join(pdf_dir, os.path.basename(pdf_filename_in_csv))\n",
    "\n",
    "                unique_download_id = os.path.splitext(os.path.basename(full_pdf_path))[0]\n",
    "                \n",
    "                if os.path.exists(full_pdf_path):\n",
    "                    if unique_download_id not in processed_ids_set:\n",
    "                        pdf_paths_to_process.append(full_pdf_path)\n",
    "                    else:\n",
    "                        skipped_due_to_already_processed += 1\n",
    "                        logging.debug(f\"PDF '{os.path.basename(full_pdf_path)}' (ID da página: {unique_download_id}) já foi processado. Pulando.\")\n",
    "                else:\n",
    "                    skipped_due_to_non_existence += 1\n",
    "                    logging.warning(f\"PDF '{os.path.basename(full_pdf_path)}' listado no CSV mas não encontrado em '{pdf_dir}'. Pulando.\")\n",
    "        else:\n",
    "             logging.debug(f\"Linha de registro com ID '{row['id']}' não tem 'downloaded_pdf_filenames' válido ou está vazia.\")\n",
    "\n",
    "    pdf_paths_to_process.sort()\n",
    "\n",
    "    if not pdf_paths_to_process:\n",
    "        logging.warning(\"Nenhum arquivo PDF novo ou não processado encontrado para conversão. Encerrando.\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"Total de {len(pdf_paths_to_process)} PDFs elegíveis para conversão.\")\n",
    "\n",
    "    global_metadata_list: List[Dict] = []\n",
    "    processed_ids_lock = threading.Lock()\n",
    "    metadata_list_lock = threading.Lock()\n",
    "\n",
    "    # 5. Usar ThreadPoolExecutor para paralelizar a conversão\n",
    "    print(f\"\\n**Iniciando extração de imagens de PDFs com {num_workers} workers (concurrent.futures)...**\")\n",
    "    logging.info(f\"Iniciando ThreadPoolExecutor com {num_workers} workers.\")\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = {executor.submit(process_single_pdf_task, \n",
    "                                   pdf_path, \n",
    "                                   jpg_output_dir, \n",
    "                                   records_df, \n",
    "                                   processed_ids_lock, \n",
    "                                   processed_ids_set, \n",
    "                                   metadata_list_lock, \n",
    "                                   global_metadata_list, \n",
    "                                   dpi): pdf_path \n",
    "                   for pdf_path in pdf_paths_to_process}\n",
    "\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), \n",
    "                           total=len(pdf_paths_to_process), \n",
    "                           desc=\"Progresso da Conversão JPG\"):\n",
    "            try:\n",
    "                future.result() \n",
    "            except Exception as exc:\n",
    "                pdf_path = futures[future]\n",
    "                logging.error(f\"'{pdf_path}' gerou uma exceção: {exc}\")\n",
    "                logging.error(traceback.format_exc())\n",
    "\n",
    "    print(\"\\n--- **Processo de Extração de Imagens Concluído!** ---\")\n",
    "    logging.info(\"Processo de extração de imagens concluído.\")\n",
    "\n",
    "    # 6. Salvar os metadados finais em CSV\n",
    "    if not global_metadata_list:\n",
    "        print(\"**Nenhuma imagem JPG foi gerada ou metadados foram coletados.**\")\n",
    "        logging.warning(\"Nenhuma imagem JPG gerada ou metadados coletados ao final do processo.\")\n",
    "    else:\n",
    "        df_images_metadata = pd.DataFrame(global_metadata_list)\n",
    "        print(\"\\n**DataFrame de Metadados de Imagens Gerado:**\")\n",
    "        print(f\"Total de {len(df_images_metadata)} registros de imagem com metadados.\")\n",
    "        display(df_images_metadata.head())\n",
    "\n",
    "        df_images_metadata.to_csv(metadata_csv_path, index=False)\n",
    "        print(f\"\\n**Metadados das imagens salvas em '{metadata_csv_path}'**\")\n",
    "        logging.info(f\"Metadados das imagens salvas em '{metadata_csv_path}'.\")\n",
    "\n",
    "    # 7. Resumo Final\n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    logging.info(\"--- PROCESSO DE CONVERSÃO DE PDF PARA JPG FINALIZADO ---\")\n",
    "    logging.info(f\"Tempo total de execução: {duration}\")\n",
    "\n",
    "    total_jpgs_generated = len([f for f in os.listdir(jpg_output_dir) if f.lower().endswith('.jpg')])\n",
    "    logging.info(f\"Total de arquivos JPG gerados/existentes em '{jpg_output_dir}': {total_jpgs_generated}\")\n",
    "\n",
    "    print(\"\\nProcesso de conversão concluído. Verifique os logs para detalhes.\")\n",
    "    print(f\"Arquivos JPG salvos em: {jpg_output_dir}\")\n",
    "    print(f\"Metadados salvos em: {metadata_csv_path}\")\n",
    "    print(\"\\n--- PROCESSAMENTO FINALIZADO ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c4aa98-f6c6-4f67-bed7-84a9c7f9ee1e",
   "metadata": {},
   "source": [
    "### 6. Execução do Processo de Conversão\n",
    "\n",
    "Esta seção demonstra como iniciar o processo de conversão de PDFs para JPGs e extração de metadados. \n",
    "\n",
    "Para executar, basta rodar a célula abaixo.\n",
    "\n",
    "**Certifique-se** que: \n",
    "1. As bibliotecas pdf2image e pandas estão instaladas\n",
    "```python\n",
    "pip install pdf2image pandas\n",
    "```\n",
    "2. A biblioteca Poppler esteja instalada em seu sistema (consulte a documentação do pdf2image para obter instruções de instalação específicas para o seu sistema operacional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f4684-4593-44ec-a72e-0d285b1d3734",
   "metadata": {},
   "outputs": [],
   "source": [
    "    main(\n",
    "        pdf_dir=PDF_DIR,\n",
    "        jpg_output_dir=JPG_OUTPUT_DIR,\n",
    "        records_csv_path=RECORDS_CSV,\n",
    "        metadata_csv_path=METADATA_CSV,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        dpi=DPI\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64dd9c8-abb4-43fc-8e72-e592d7c3fa7a",
   "metadata": {},
   "source": [
    "### 7. Verificação de Integridade da Conversão\n",
    "\n",
    "Esta função auxiliar é essencial para validar a qualidade e completude do processo de conversão. \n",
    "\n",
    "Ela verifica se todos os PDFs listados no seu CSV original que deveriam ter sido convertidos realmente têm entradas correspondentes no arquivo de metadados gerado. \n",
    "\n",
    "Além disso, ela identifica quaisquer entradas no CSV de metadados que não correspondem a um ID esperado do CSV de registros, ajudando a detectar inconsistências."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f1f9fa-5425-4860-ad23-7a81fd607929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_conversion_integrity(records_csv_path: str, metadata_csv_path: str) -> Tuple[set, List[Dict]]:\n",
    "    expected_download_ids: set = set()\n",
    "    try:\n",
    "        records_df = pd.read_csv(records_csv_path)\n",
    "        \n",
    "        required_records_cols = ['id', 'downloaded_pdf_filenames']\n",
    "        for col in required_records_cols:\n",
    "            if col not in records_df.columns:\n",
    "                logging.error(f\"Erro: Coluna essencial '{col}' não encontrada em '{records_csv_path}'.\")\n",
    "                logging.error(\"Não é possível realizar a verificação de integridade sem as colunas corretas.\")\n",
    "                return set(), []\n",
    "\n",
    "        records_df['id'] = records_df['id'].astype(str)\n",
    "        records_df['downloaded_pdf_filenames'] = records_df['downloaded_pdf_filenames'].apply(\n",
    "            lambda x: eval(x) if pd.notna(x) and x.strip().startswith('[') else []\n",
    "        )\n",
    "\n",
    "        for _, row in records_df.iterrows():\n",
    "            for pdf_filename in row['downloaded_pdf_filenames']:\n",
    "                unique_id = os.path.splitext(os.path.basename(pdf_filename))[0]\n",
    "                expected_download_ids.add(unique_id)\n",
    "        \n",
    "        logging.info(f\"Esperados {len(expected_download_ids)} IDs de downloads únicos com base em '{records_csv_path}'.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Erro: Arquivo CSV de registros '{records_csv_path}' não encontrado. Verificação abortada.\")\n",
    "        return set(), []\n",
    "    except pd.errors.EmptyDataError:\n",
    "        logging.warning(f\"Aviso: Arquivo CSV de registros '{records_csv_path}' está vazio. Nenhuma entrada esperada.\")\n",
    "        return set(), []\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro ao carregar ou processar '{records_csv_path}' para verificação: {e}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        return set(), []\n",
    "\n",
    "    found_download_ids: set = set()\n",
    "    found_original_ids_in_metadata: set = set()\n",
    "    metadata_df: Optional[pd.DataFrame] = None\n",
    "\n",
    "    try:\n",
    "        metadata_df = pd.read_csv(metadata_csv_path, dtype={'download_id': str, 'original_article_id': str})\n",
    "        \n",
    "        required_metadata_cols = ['download_id', 'original_article_id']\n",
    "        for col in required_metadata_cols:\n",
    "            if col not in metadata_df.columns:\n",
    "                logging.error(f\"Erro: Coluna essencial '{col}' não encontrada em '{metadata_csv_path}'.\")\n",
    "                logging.error(\"Não é possível realizar a verificação de integridade sem as colunas corretas.\")\n",
    "                return expected_download_ids, []\n",
    "\n",
    "        found_download_ids = set(metadata_df['download_id'].unique())\n",
    "        found_original_ids_in_metadata = set(metadata_df['original_article_id'].unique())\n",
    "\n",
    "        logging.info(f\"Encontrados {len(found_download_ids)} IDs de downloads únicos no CSV de metadados '{metadata_csv_path}'.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.warning(f\"Aviso: Arquivo CSV de metadados '{metadata_csv_path}' não encontrado. Assumindo que todos os PDFs estão faltando.\")\n",
    "        return expected_download_ids, []\n",
    "    except pd.errors.EmptyDataError:\n",
    "        logging.warning(f\"Aviso: Arquivo CSV de metadados '{metadata_csv_path}' está vazio. Nenhuma imagem foi processada.\")\n",
    "        return expected_download_ids, []\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro ao carregar ou processar '{metadata_csv_path}' para verificação: {e}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        return expected_download_ids, []\n",
    "\n",
    "    missing_download_ids = expected_download_ids - found_download_ids\n",
    "\n",
    "    extra_metadata_entries: List[Dict] = []\n",
    "    if records_df is not None and metadata_df is not None and not records_df.empty and not metadata_df.empty:\n",
    "        valid_original_ids_from_records = set(records_df['id'].unique())\n",
    "        \n",
    "        extra_metadata_df = metadata_df[~metadata_df['original_article_id'].isin(valid_original_ids_from_records)]\n",
    "        extra_metadata_entries = extra_metadata_df.to_dict('records')\n",
    "        \n",
    "        if extra_metadata_entries:\n",
    "            logging.warning(f\"Encontradas {len(extra_metadata_entries)} entradas 'extras' no CSV de metadados.\")\n",
    "        \n",
    "    logging.info(\"Verificação de integridade concluída.\")\n",
    "    return missing_download_ids, extra_metadata_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d732845e-b344-44c2-951c-e9fa2c56eee6",
   "metadata": {},
   "source": [
    "Depois de rodar a célula principal (main_conversion_process), você pode rodar esta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fe63fc-0c20-459d-9043-c812af114249",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ids, extra_entries = check_conversion_integrity(RECORDS_CSV, METADATA_CSV)\n",
    "\n",
    "if missing_ids:\n",
    "    print(f\"\\n--- ATENÇÃO: {len(missing_ids)} PDFs não foram processados ---\")\n",
    "    print(\"IDs de downloads faltando:\", missing_ids)\n",
    "else:\n",
    "    print(\"\\nTodos os PDFs esperados foram encontrados no CSV de metadados.\")\n",
    "\n",
    "if extra_entries:\n",
    "    print(f\"\\n--- ATENÇÃO: {len(extra_entries)} entradas 'extras' encontradas no CSV de metadados ---\")\n",
    "    print(\"Exemplos de entradas extras (primeiras 5):\")\n",
    "    for entry in extra_entries[:5]:\n",
    "        print(entry)\n",
    "else:\n",
    "    print(\"\\nNenhuma entrada 'extra' encontrada no CSV de metadados.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
